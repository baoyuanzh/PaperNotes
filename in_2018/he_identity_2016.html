
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Identity Mappings in Deep Residual Networks &#8212; Paper Notes  documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Deep Residual Learning for Image Recognition" href="he_deep_2015.html" />
    <link rel="prev" title="Nominal, Ordinal, Interval, and Ratio Typologies are Misleading" href="velleman_norminal_1993.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="identity-mappings-in-deep-residual-networks">
<h1>Identity Mappings in Deep Residual Networks<a class="headerlink" href="#identity-mappings-in-deep-residual-networks" title="Permalink to this headline">¶</a></h1>
<p>K. He, X. Zhang, S. Ren, and J. Sun, <em>“Identity Mappings in Deep Residual Networks,”</em> in Computer Vision – ECCV 2016, 2016, pp. 630–645.</p>
<p>本文的主旨：</p>
<blockquote>
<div>The forward and backward signals can be directly propagated from <em>one block to</em> <strong>any</strong> <em>other block</em>, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the <em>importance of these identity mappings</em>.</div></blockquote>
<p>以下是一些用于测试的残差网络单元， Residual units</p>
<img alt="../_images/various_residual_units.png" src="../_images/various_residual_units.png" />
<p>残差网络单元的数学表示，</p>
<div class="math">
\[\begin{split}\begin{split}
\mathbf{y}_{l} &amp;= \mathcal{h}(\mathbf{x}_{l}) + \mathcal{F}(\mathbf{x}_{l}, \mathcal{W}_{l})\\
\mathbf{x}_{l+1} &amp;= \mathcal{f}(\mathbf{y}_{l})
\end{split}\end{split}\]</div>
<p>通过使用不同的 <span class="math">\(\mathcal{h}(\cdot)\)</span> 和 <span class="math">\(\mathcal{f}(\cdot)\)</span> 我们就可以构造出不同的残差单元。</p>
<p>这里有一个比较有意思的观点，通常来讲，我们把 BN 或者 ReLU 都是当作 post-activation 来用，但是当，网络一层一层搭起来以后，夹在两个参数层中间的 ReLU 或 BN 既可以当作是上一层的 “post-activation” 也可以当作是下一层的 “pre-activation”， 除了开头和结尾的情况例外。
因此这里就以 pre-activation 的方式来看待这些 nonlinear mapping.</p>
<p>本文中使用 <span class="math">\(\mathcal{h} = \mathcal{f} = \text{identity mapping}\)</span> 来作 skip connections 和 after-addition activation. 这时，就有</p>
<div class="math">
\[\begin{split}\mathbf{x}_{l+1} = \mathbf{x}_{l} + \mathcal{F}(\mathbf{x}_{l}, \mathcal{W}_{l})\\\end{split}\]</div>
<p>有了该简化，就可以看到在前向传播中，有 signals can be directly propagated from one block to any other block,</p>
<div class="math">
\[\begin{split}\mathbf{x}_{L} = \mathbf{x}_{l} + \sum_{i=l}^{L-1} \mathcal{F}(\mathbf{x}_{i}, \mathcal{W}_{i})\\\end{split}\]</div>
<p>在后向传播中，有 signals can be directly propagated from one block to any other block,</p>
<div class="math">
\[\begin{split}\frac{\partial \mathcal{E}}{\partial \mathbf{x}_{l}} = \frac{\partial \mathcal{E}}{\partial \mathbf{x}_{L}}\frac{\partial \mathbf{x}_{L}}{\partial \mathbf{x}_{l}} = \frac{\partial \mathcal{E}}{\partial \mathbf{x}_{L}}\left(1 + \frac{\partial}{\partial \mathbf{x}_{l}}\sum_{i=l}^{L-1} \mathcal{F}(\mathbf{x}_{i}, \mathcal{W}_{i}\right)\\\end{split}\]</div>
<p><strong>这里重要的是，不是对于特定的</strong> <span class="math">\(L\)</span> <strong>和</strong> <span class="math">\(l\)</span> <strong>才成立上述结论，而是对于任意的</strong> <span class="math">\(L\)</span> <strong>和</strong> <span class="math">\(l\)</span> <strong>都成立.</strong> <em>这可能是使得 ResNet 比较好训练的原因。</em></p>
<p>思考：这样的话 Dense Net 似乎就有点尴尬了？</p>
<p>另外，如果 skip connections <span class="math">\(\mathcal{h}\)</span> 不是 identity mapping 的话，即使是最简单的线性改变</p>
<div class="math">
\[\mathbf{x}_{l+1} = \lambda_{l}\mathbf{x}_{l} + \mathcal{F}(\mathbf{x}_{l}, \mathcal{W}_{l})\]</div>
<p>也会产生诸如梯度爆炸或者消失的问题。在梯度消失的情形下，这相当于 <strong>强迫信息通过参数层</strong> 传递。
在更复杂的情形下，文章通过实验的方法说明，非 identity mapping 会给优化带来困难。
Shortcut 代表了信息最容易通过的渠道，因此，在 shortcut 中使用复杂的 mapping 会阻碍优化。</p>
<img alt="../_images/complex_shortcut.png" src="../_images/complex_shortcut.png" />
<p>事实上，其中某些复杂的 shortcut 包含了 identity mapping ，因此该模型的表达能力实际上提升了，然而最终的结果却表现不好，说明复杂的 shortcut 影响的是优化的过程。</p>
<p>在讨论了 identity mapping shortcut 的重要性之后，文章再讨论了 after-addition 之后的 activation 的影响。
如果我们不是在原来 ReLU 的基础上简化，而是增加一个 BN 层，那么结果将变得更差，这说明，在 after-addition 的 activation 的复杂化也会影响优化的难度。
因此，我们应该试着简化这部分 activation 而不是怎加复杂性。</p>
<p>文章首先考虑了把原先的 ReLU 只作用到右侧的参数层上，然后通过转换视角，把 post-activation 当做 pre-activation 看待来处理问题，如下图所示。</p>
<img alt="../_images/pre_activation.png" src="../_images/pre_activation.png" />
<p>文章比较了把 BN 和 ReLU 都作为 pre-activation 和只把 ReLU 作为 pre-activation 的 configuration ，这对于有分支的网络来说是不同的，结果显示前者比较好，可能是以为后者的第一个 ReLU 因为上面的 addition 的关系，没有享受到 BN 带来的好处。</p>
<p>本文清晰的展示了作者的思路，结果也显示了 pre-activation 确实都改进了结果。
我也比较认同作者的观点，在 shortcut 那一条路上保持 clean 的 identity mapping 以方便信息在前向和后向传播中，能够毫无阻碍的从任意一层传递到任意另一层，确实可以在一定程度上是的优化的难度降低。</p>
<p>文章关于 pre-activation 的两点总结：</p>
<ul class="simple">
<li><dl class="first docutils">
<dt><strong>Ease of optimization</strong></dt>
<dd><ul class="first last">
<li>相比原先的 ResNet 网络，pre-activation 版本在优化初期 loss 就下降得很快</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><strong>Reducing overfitting</strong></dt>
<dd><ul class="first last">
<li>training error 比原先的 ResNet 的网络高，但是 test error 却更低，这是一个比较有利的证据，可能的原因是 BN 在原先的 ResNet 中并没有对相加后的信号做 Normalization 处理，而在 pre-activation 的版本中，Normalization 则得到了充分的利用，每个分支的 weight layer 前都有 BN ，因此， BN 的 regularization 的作用得到了充分的发挥。</li>
</ul>
</dd>
</dl>
</li>
</ul>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Paper Notes</a></h1>








<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="toc.html">2018</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="velleman_norminal_1993.html">Nominal, Ordinal, Interval, and Ratio Typologies are Misleading</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Identity Mappings in Deep Residual Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="he_deep_2015.html">Deep Residual Learning for Image Recognition</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="toc.html">2018</a><ul>
      <li>Previous: <a href="velleman_norminal_1993.html" title="previous chapter">Nominal, Ordinal, Interval, and Ratio Typologies are Misleading</a></li>
      <li>Next: <a href="he_deep_2015.html" title="next chapter">Deep Residual Learning for Image Recognition</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2018, ZHANG Baoyuan.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.6.6</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.10</a>
      
      |
      <a href="../_sources/in_2018/he_identity_2016.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>